{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c0e494bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "from numpy import *\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as tf\n",
    "#from torchvision.transforms import F as tf\n",
    "\n",
    "import glob\n",
    "from PIL import Image\n",
    "\n",
    "import sys\n",
    "import time\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = \"retina\"\n",
    "\n",
    "\n",
    "import random\n",
    "def setup_seed(seed):\n",
    "     torch.manual_seed(seed)\n",
    "     torch.cuda.manual_seed_all(seed)\n",
    "     np.random.seed(seed)\n",
    "     random.seed(seed)\n",
    "     torch.backends.cudnn.deterministic = True\n",
    "# set random seed\n",
    "setup_seed(88)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9584424e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#————————————————————————————————————————————————————————————————————————————————————————\n",
    "#  define transform\n",
    "\n",
    "transform = transforms.Compose([\n",
    "                                # transforms.Resize((512,512)), # resize 成 512,512\n",
    "                                transforms.ToTensor(), # \n",
    "])\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, imgs_path, annos_path,cropp=False):\n",
    "        self.imgs_path = imgs_path\n",
    "        self.annos_path = annos_path\n",
    "        self.cropp = cropp\n",
    "    def crop(self,img,label,size):\n",
    "        t = transforms.RandomCrop.get_params(img=img,output_size=(size,size))\n",
    "        img = tf.crop(img, *t)\n",
    "        label = tf.crop(label, *t)\n",
    "        return img,label\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img = self.imgs_path[index]\n",
    "        anno = self.annos_path[index]\n",
    "\n",
    "        pil_img = Image.open(img)#.convert('RGB')\n",
    "        anno_img = Image.open(anno).convert('L')\n",
    "        \n",
    "        if self.cropp:\n",
    "            pil_img,anno_img = self.crop(pil_img,anno_img,128)\n",
    "        \n",
    "        img_tensor = transform(pil_img)\n",
    "        img_tensor = img_tensor.to(torch.float)\n",
    "\n",
    "        \n",
    "        anno_tensor = transform(anno_img)\n",
    "        # the image is [256 256 1] \n",
    "        \n",
    "        \n",
    "        #anno_tensor[anno_tensor>0] = 1 \n",
    "        anno_tensor = anno_tensor*255. # set the value of the label from 0.. 1 2 ... ，\n",
    "\n",
    "        # change the image to [256,256]\n",
    "        anno_tensor = torch.squeeze(anno_tensor).type(torch.long) \n",
    "\n",
    "        return img_tensor, anno_tensor\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c253ce1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#                           Unet \n",
    "#——————————————————————————————————————————————————————————————————————\n",
    "#\n",
    "# build the Unet model \n",
    "###\n",
    "#    Unet\n",
    "class Downsample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Downsample, self).__init__()\n",
    "        self.conv_relu = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "                                       nn.BatchNorm2d(out_channels),\n",
    "                                       nn.ReLU(inplace=True),\n",
    "                                       nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "                                       nn.BatchNorm2d(out_channels),\n",
    "                                       nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.pool = nn.MaxPool2d(kernel_size = 2)\n",
    "    \n",
    "    def forward(self, x, is_pool=True):\n",
    "\n",
    "        if is_pool: # \n",
    "            x = self.pool(x)\n",
    "\n",
    "        x = self.conv_relu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# up-sample layer\n",
    "class Upsample(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(Upsample, self).__init__()\n",
    "        self.conv_relu = nn.Sequential(nn.Conv2d(2*channels, channels, kernel_size=3, padding=1),\n",
    "                                       nn.BatchNorm2d(channels),\n",
    "                                       nn.ReLU(inplace=True),\n",
    "                                       \n",
    "                                       nn.Conv2d(channels, channels, kernel_size=3, padding=1),\n",
    "                                       nn.BatchNorm2d(channels),\n",
    "                                       nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.upconv = nn.Sequential(nn.ConvTranspose2d(channels, channels//2, kernel_size=3, stride=2,padding=1,output_padding=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_relu(x)\n",
    "        x = self.upconv(x)\n",
    "        return x\n",
    "\n",
    "# \n",
    "class Unet_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Unet_model, self).__init__()\n",
    "        self.down1 = Downsample(1,64) # if the input image is in 3 channel, change 1 to 3\n",
    "        self.down2 = Downsample(64,128)\n",
    "        self.down3 = Downsample(128,256)\n",
    "        self.down4 = Downsample(256,512)\n",
    "        self.down5 = Downsample(512,1024)\n",
    "\n",
    "        self.up = nn.Sequential(nn.ConvTranspose2d(1024,512,kernel_size=3,stride=2,padding=1,output_padding=1),\n",
    "                                #nn.Dropout(p=0.5),\n",
    "                                nn.BatchNorm2d(512),\n",
    "                                nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.up1 = Upsample(512)\n",
    "        self.up2 = Upsample(256)\n",
    "        self.up3 = Upsample(128)\n",
    "\n",
    "        self.conv_2 = Downsample(128,64)\n",
    "\n",
    "        self.last = nn.Sequential(nn.Conv2d(64,26,kernel_size=1),\n",
    "                                  #nn.Dropout(p=0.5)\n",
    "                                  ) # 26=25+1 25 classes\n",
    "    \n",
    "    def forward(self, input):\n",
    "        x1 = self.down1(input, is_pool=False)\n",
    "        x2 = self.down2(x1)\n",
    "        x3 = self.down3(x2)\n",
    "        x4 = self.down4(x3)\n",
    "        x5 = self.down5(x4)\n",
    "\n",
    "        x5 = self.up(x5)\n",
    "\n",
    "        x5 = torch.cat([x4,x5], dim=1) #\n",
    "        x5 = self.up1(x5) \n",
    "\n",
    "        x5 = torch.cat([x3,x5], dim=1)\n",
    "        x5 = self.up2(x5) \n",
    "\n",
    "        x5 = torch.cat([x2,x5], dim=1)\n",
    "        x5 = self.up3(x5) \n",
    "\n",
    "        x5 = torch.cat([x1,x5], dim=1)\n",
    "\n",
    "        x5 = self.conv_2(x5, is_pool=False)\n",
    "\n",
    "        x5 = self.last(x5)\n",
    "\n",
    "        return x5\n",
    "\n",
    "    \n",
    "# load the weight trained before\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0b47d4ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MB_R:  521\n",
      "521\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n",
      "(26, 256, 256)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# load weight to model\n",
    "PATH = './.............pth' # locate to the weights path\n",
    "model =  Unet_model()\n",
    "model.load_state_dict(torch.load(PATH,map_location=torch.device('cpu') ))\n",
    "\n",
    "# load data\n",
    "# read the png files\n",
    "MB_R = glob.glob('........./*.png');MB_R = sorted(MB_R);\n",
    "print('MB_R: ',len(MB_R))\n",
    "\n",
    "# read the label files\n",
    "MB_anno_R = glob.glob('......../*.png');print(len(MB_anno_R));\n",
    "\n",
    "test_images = MB_R\n",
    "test_images = sorted(test_images) \n",
    "test_annos = MB_anno_R\n",
    "test_annos = sorted(test_annos) \n",
    "test_dataset = Dataset(test_images,test_annos)\n",
    "\n",
    "\n",
    "# \n",
    "test_dataloader = data.DataLoader(test_dataset,\n",
    "                                   batch_size = 1,\n",
    "                                   shuffle=False,\n",
    "                                  num_workers=0) # \n",
    "\n",
    "model.eval()\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# save to .npy file [26,256,256]\n",
    "i = 0\n",
    "with torch.no_grad():\n",
    "    for x, y in test_dataloader:\n",
    "        i = i+1\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        y_pred = model(x)\n",
    "        y_pred = y_pred.squeeze().cpu().numpy()\n",
    "\n",
    "\n",
    "        if i < 10:\n",
    "          filename = './ '.npy'\n",
    "          np.save(filename,y_pred)\n",
    "        elif i>=10 and i<=99:\n",
    "          filename = './' '.npy'\n",
    "          np.save(filename,y_pred)\n",
    "        else:\n",
    "          filename = './' '.npy'\n",
    "          np.save(filename,y_pred)\n",
    "\n",
    "        \n",
    "\n",
    "        print(y_pred.shape)\n",
    "        # \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f7c6dcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu",
   "language": "python",
   "name": "pytorch_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
